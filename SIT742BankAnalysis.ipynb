{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SIT742BankAnalysis.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/resh1604/SIT742/blob/master/SIT742BankAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOUUQCsm8ecf",
        "colab_type": "text"
      },
      "source": [
        "# SIT742: Modern Data Science \n",
        "**(Assessment Task 02: Bank Marketing Data Analytics)**\n",
        "\n",
        "---\n",
        "- Materials in this module include resources collected from various open-source online repositories.\n",
        "- You are free to use, change and distribute this package.\n",
        "\n",
        "Prepared by **SIT742 Teaching Team**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Project Group Information:**\n",
        "\n",
        "- Names:\n",
        "- Student IDs:\n",
        "- Emails:\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "eASNvREtBU9G"
      },
      "source": [
        "## 1. Import Spark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "code",
        "colab_type": "code",
        "id": "duCJfMrnGu00",
        "outputId": "ed754f35-4453-4936-fd12-f0b06f3e9c44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#!wget -q http://www-us.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "#!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "#!pip install -q findspark\n",
        "\n",
        "#import os\n",
        "#os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "#os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\"\n",
        "\n",
        "!pip install wget\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.0-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.0-bin-hadoop2.7\""
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aGOo805LA-fM",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "K7bEtO_fBZmE"
      },
      "source": [
        "## 2. Read and check data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5qcOpBu8ecq",
        "colab_type": "code",
        "outputId": "2c174d54-a209-4eb7-fc6a-e7958c39697c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "!pip install wget  "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.6/dist-packages (3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6c6Lla88ect",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wget\n",
        "\n",
        "link_to_data = 'https://github.com/tulip-lab/sit742/raw/master/Assessment/2019/data/bank.csv'\n",
        "DataSet = wget.download(link_to_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR55yOYn8ecw",
        "colab_type": "code",
        "outputId": "f00200f4-a266-4afd-90fb-a5326d0bbb08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'bank (1).csv'\t spark-2.4.0-bin-hadoop2.7\n",
            "'bank (2).csv'\t spark-2.4.0-bin-hadoop2.7.tgz\n",
            " bank.csv\t spark-2.4.0-bin-hadoop2.7.tgz.1\n",
            " sample_data\t spark-2.4.0-bin-hadoop2.7.tgz.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FQ8Ts9eZBA-M",
        "outputId": "00f20dfc-9763-4b14-94df-dc79b9e5e900",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "spark = SparkSession.builder.appName('ml-bank').getOrCreate()\n",
        "\n",
        "from pyspark.sql.types import StructType, StructField\n",
        "from pyspark.sql.types import DoubleType, IntegerType, StringType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"age\", IntegerType()),\n",
        "    StructField(\"job\", StringType()),\n",
        "    StructField(\"marital\", StringType()),\n",
        "    StructField(\"education\", StringType()),\n",
        "    StructField(\"default\", StringType()),\n",
        "    StructField(\"balance\", IntegerType()),\n",
        "    StructField(\"housing\", StringType()),\n",
        "    StructField(\"contact\", StringType()),\n",
        "    StructField(\"day\", IntegerType()),\n",
        "    StructField(\"month\", StringType()),\n",
        "    StructField(\"duration\", IntegerType()),\n",
        "    StructField(\"campaign\", IntegerType()),\n",
        "    StructField(\"pdays\", IntegerType()),\n",
        "    StructField(\"previous\", IntegerType()),\n",
        "    StructField(\"poutcome\", StringType()),\n",
        "    StructField(\"deposit\", StringType())\n",
        "])\n",
        "\n",
        "# Import the 'bank.csv' as a Spark dataframe and name it as df\n",
        "df = spark.read.format(\"csv\").\\\n",
        "        option(\"header\", \"true\").\\\n",
        "            load(\"bank.csv\")\n",
        "        \n",
        "df = df.withColumn(\"age\", df[\"age\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"balance\", df[\"balance\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"day\", df[\"day\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"duration\", df[\"duration\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"campaign\", df[\"campaign\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"pdays\", df[\"pdays\"].cast(IntegerType()))\n",
        "df = df.withColumn(\"previous\", df[\"previous\"].cast(IntegerType()))\n",
        "\n",
        "df"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[age: int, job: string, marital: string, education: string, default: string, balance: int, housing: string, loan: string, contact: string, day: int, month: string, duration: int, campaign: int, pdays: int, previous: int, poutcome: string, deposit: string]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kD7R8DK4COOb",
        "colab_type": "code",
        "outputId": "f02193bb-d1df-4bee-ed3e-1bd2fde27b50",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "df.head(1)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(age=59, job='admin.', marital='married', education='secondary', default='no', balance=2343, housing='yes', loan='no', contact='unknown', day=5, month='may', duration=1042, campaign=1, pdays=-1, previous=0, poutcome='unknown', deposit='yes')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XJGWVc0yB0UA",
        "outputId": "f9791f14-3e21-4885-ad4b-8524e5a209c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 334
        }
      },
      "source": [
        "# check data distribution\n",
        "# you may use printSchema() \n",
        "df.printSchema()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- age: integer (nullable = true)\n",
            " |-- job: string (nullable = true)\n",
            " |-- marital: string (nullable = true)\n",
            " |-- education: string (nullable = true)\n",
            " |-- default: string (nullable = true)\n",
            " |-- balance: integer (nullable = true)\n",
            " |-- housing: string (nullable = true)\n",
            " |-- loan: string (nullable = true)\n",
            " |-- contact: string (nullable = true)\n",
            " |-- day: integer (nullable = true)\n",
            " |-- month: string (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- campaign: integer (nullable = true)\n",
            " |-- pdays: integer (nullable = true)\n",
            " |-- previous: integer (nullable = true)\n",
            " |-- poutcome: string (nullable = true)\n",
            " |-- deposit: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wKQMhrtFChHa"
      },
      "source": [
        "## 3. Select features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VU5xMqN_RyM2",
        "outputId": "a41cb15c-82bc-4f9a-df91-0b132c0b623b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# select features ('age', 'job', 'marital', 'education', 'default', 'balance', 'housing', 'loan', 'campaign', 'pdays', 'previous', 'poutcome', 'deposit') as df2\n",
        "columns = ['age',\n",
        "           'job', \n",
        "           'marital',\n",
        "           'education',\n",
        "           'default', \n",
        "           'balance', \n",
        "           'housing',\n",
        "           'loan',\n",
        "           'campaign',\n",
        "           'pdays', \n",
        "           'previous',\n",
        "           'poutcome',\n",
        "           'deposit']\n",
        "df2 = df.select([c for c in df.columns if c in columns])\n",
        "df2.head(1)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(age=59, job='admin.', marital='married', education='secondary', default='no', balance=2343, housing='yes', loan='no', campaign=1, pdays=-1, previous=0, poutcome='unknown', deposit='yes')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vwF5sqRYa_eI",
        "colab": {}
      },
      "source": [
        "# remove invalid rows/records using spark.sql \n",
        "df2 = df2.na.drop()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A_T8qvxzR-oI",
        "outputId": "7d796a68-2970-48fd-a4a0-1dd5ac43b007",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "# convert categorical features to numeric features  using One hot encoding, \n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler, OneHotEncoder\n",
        "\n",
        "cols = [\"job\", \n",
        "        \"marital\",\n",
        "        \"education\",\n",
        "        \"default\",\n",
        "        \"housing\",\n",
        "        \"loan\",\n",
        "        \"poutcome\",\n",
        "        \"deposit\"]\n",
        "\n",
        "for col in cols:\n",
        "  stringIndexer = StringIndexer(inputCol=col, outputCol=\"{}Index\".format(col))\n",
        "  model = stringIndexer.fit(df2)\n",
        "  indexed = model.transform(df2)\n",
        "\n",
        "  encoder = OneHotEncoder(inputCol=\"{}Index\".format(col), outputCol=\"{}Vec\".format(col))\n",
        "  df2 = encoder.transform(indexed)\n",
        "  \n",
        "df2.show()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+--------+--------------+------------+-------------+--------------+-------------+------------+-------------+------------+-------------+---------+-------------+-------------+-------------+------------+----------+\n",
            "|age|        job| marital|education|default|balance|housing|loan|campaign|pdays|previous|poutcome|deposit|jobIndex|        jobVec|maritalIndex|   maritalVec|educationIndex| educationVec|defaultIndex|   defaultVec|housingIndex|   housingVec|loanIndex|      loanVec|poutcomeIndex|  poutcomeVec|depositIndex|depositVec|\n",
            "+---+-----------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+--------+--------------+------------+-------------+--------------+-------------+------------+-------------+------------+-------------+---------+-------------+-------------+-------------+------------+----------+\n",
            "| 59|     admin.| married|secondary|     no|   2343|    yes|  no|       1|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 56|     admin.| married|secondary|     no|     45|     no|  no|       1|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 41| technician| married|secondary|     no|   1270|    yes|  no|       1|   -1|       0| unknown|    yes|     2.0|(11,[2],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 55|   services| married|secondary|     no|   2476|    yes|  no|       1|   -1|       0| unknown|    yes|     4.0|(11,[4],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 54|     admin.| married| tertiary|     no|    184|     no|  no|       2|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         0.0|(2,[0],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 42| management|  single| tertiary|     no|      0|    yes| yes|       2|   -1|       0| unknown|    yes|     0.0|(11,[0],[1.0])|         1.0|(2,[1],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      1.0|    (1,[],[])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 56| management| married| tertiary|     no|    830|    yes| yes|       1|   -1|       0| unknown|    yes|     0.0|(11,[0],[1.0])|         0.0|(2,[0],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      1.0|    (1,[],[])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 60|    retired|divorced|secondary|     no|    545|    yes|  no|       1|   -1|       0| unknown|    yes|     5.0|(11,[5],[1.0])|         2.0|    (2,[],[])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 37| technician| married|secondary|     no|      1|    yes|  no|       1|   -1|       0| unknown|    yes|     2.0|(11,[2],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 28|   services|  single|secondary|     no|   5090|    yes|  no|       3|   -1|       0| unknown|    yes|     4.0|(11,[4],[1.0])|         1.0|(2,[1],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 38|     admin.|  single|secondary|     no|    100|    yes|  no|       1|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         1.0|(2,[1],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 30|blue-collar| married|secondary|     no|    309|    yes|  no|       2|   -1|       0| unknown|    yes|     1.0|(11,[1],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 29| management| married| tertiary|     no|    199|    yes| yes|       4|   -1|       0| unknown|    yes|     0.0|(11,[0],[1.0])|         0.0|(2,[0],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      1.0|    (1,[],[])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 46|blue-collar|  single| tertiary|     no|    460|    yes|  no|       2|   -1|       0| unknown|    yes|     1.0|(11,[1],[1.0])|         1.0|(2,[1],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 31| technician|  single| tertiary|     no|    703|    yes|  no|       2|   -1|       0| unknown|    yes|     2.0|(11,[2],[1.0])|         1.0|(2,[1],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 35| management|divorced| tertiary|     no|   3837|    yes|  no|       1|   -1|       0| unknown|    yes|     0.0|(11,[0],[1.0])|         2.0|    (2,[],[])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 32|blue-collar|  single|  primary|     no|    611|    yes|  no|       3|   -1|       0| unknown|    yes|     1.0|(11,[1],[1.0])|         1.0|(2,[1],[1.0])|           2.0|(3,[2],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 49|   services| married|secondary|     no|     -8|    yes|  no|       1|   -1|       0| unknown|    yes|     4.0|(11,[4],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 41|     admin.| married|secondary|     no|     55|    yes|  no|       2|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "| 49|     admin.|divorced|secondary|     no|    168|    yes| yes|       1|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         2.0|    (2,[],[])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      1.0|    (1,[],[])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|\n",
            "+---+-----------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+--------+--------------+------------+-------------+--------------+-------------+------------+-------------+------------+-------------+---------+-------------+-------------+-------------+------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pOqHERYJCQuC"
      },
      "source": [
        "### 3.1 normalisation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5p-VF7zqJNr5",
        "colab_type": "code",
        "outputId": "b9d3a616-356b-4688-b7b0-ddbb6f641f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        }
      },
      "source": [
        "numeric_variables = [\n",
        "    'age',\n",
        "    'balance',\n",
        "    'pdays',\n",
        "    'previous'\n",
        "]\n",
        "\n",
        "\n",
        "vecAssembler = VectorAssembler(inputCols=numeric_variables, outputCol=\"nums\")\n",
        "df2 = vecAssembler.transform(df2)\n",
        "df2.show()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+-----------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+--------+--------------+------------+-------------+--------------+-------------+------------+-------------+------------+-------------+---------+-------------+-------------+-------------+------------+----------+--------------------+\n",
            "|age|        job| marital|education|default|balance|housing|loan|campaign|pdays|previous|poutcome|deposit|jobIndex|        jobVec|maritalIndex|   maritalVec|educationIndex| educationVec|defaultIndex|   defaultVec|housingIndex|   housingVec|loanIndex|      loanVec|poutcomeIndex|  poutcomeVec|depositIndex|depositVec|                nums|\n",
            "+---+-----------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+--------+--------------+------------+-------------+--------------+-------------+------------+-------------+------------+-------------+---------+-------------+-------------+-------------+------------+----------+--------------------+\n",
            "| 59|     admin.| married|secondary|     no|   2343|    yes|  no|       1|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[59.0,2343.0,-1.0...|\n",
            "| 56|     admin.| married|secondary|     no|     45|     no|  no|       1|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[56.0,45.0,-1.0,0.0]|\n",
            "| 41| technician| married|secondary|     no|   1270|    yes|  no|       1|   -1|       0| unknown|    yes|     2.0|(11,[2],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[41.0,1270.0,-1.0...|\n",
            "| 55|   services| married|secondary|     no|   2476|    yes|  no|       1|   -1|       0| unknown|    yes|     4.0|(11,[4],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[55.0,2476.0,-1.0...|\n",
            "| 54|     admin.| married| tertiary|     no|    184|     no|  no|       2|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         0.0|(2,[0],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         0.0|(1,[0],[1.0])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[54.0,184.0,-1.0,...|\n",
            "| 42| management|  single| tertiary|     no|      0|    yes| yes|       2|   -1|       0| unknown|    yes|     0.0|(11,[0],[1.0])|         1.0|(2,[1],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      1.0|    (1,[],[])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])| [42.0,0.0,-1.0,0.0]|\n",
            "| 56| management| married| tertiary|     no|    830|    yes| yes|       1|   -1|       0| unknown|    yes|     0.0|(11,[0],[1.0])|         0.0|(2,[0],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      1.0|    (1,[],[])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[56.0,830.0,-1.0,...|\n",
            "| 60|    retired|divorced|secondary|     no|    545|    yes|  no|       1|   -1|       0| unknown|    yes|     5.0|(11,[5],[1.0])|         2.0|    (2,[],[])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[60.0,545.0,-1.0,...|\n",
            "| 37| technician| married|secondary|     no|      1|    yes|  no|       1|   -1|       0| unknown|    yes|     2.0|(11,[2],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])| [37.0,1.0,-1.0,0.0]|\n",
            "| 28|   services|  single|secondary|     no|   5090|    yes|  no|       3|   -1|       0| unknown|    yes|     4.0|(11,[4],[1.0])|         1.0|(2,[1],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[28.0,5090.0,-1.0...|\n",
            "| 38|     admin.|  single|secondary|     no|    100|    yes|  no|       1|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         1.0|(2,[1],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[38.0,100.0,-1.0,...|\n",
            "| 30|blue-collar| married|secondary|     no|    309|    yes|  no|       2|   -1|       0| unknown|    yes|     1.0|(11,[1],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[30.0,309.0,-1.0,...|\n",
            "| 29| management| married| tertiary|     no|    199|    yes| yes|       4|   -1|       0| unknown|    yes|     0.0|(11,[0],[1.0])|         0.0|(2,[0],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      1.0|    (1,[],[])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[29.0,199.0,-1.0,...|\n",
            "| 46|blue-collar|  single| tertiary|     no|    460|    yes|  no|       2|   -1|       0| unknown|    yes|     1.0|(11,[1],[1.0])|         1.0|(2,[1],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[46.0,460.0,-1.0,...|\n",
            "| 31| technician|  single| tertiary|     no|    703|    yes|  no|       2|   -1|       0| unknown|    yes|     2.0|(11,[2],[1.0])|         1.0|(2,[1],[1.0])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[31.0,703.0,-1.0,...|\n",
            "| 35| management|divorced| tertiary|     no|   3837|    yes|  no|       1|   -1|       0| unknown|    yes|     0.0|(11,[0],[1.0])|         2.0|    (2,[],[])|           1.0|(3,[1],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[35.0,3837.0,-1.0...|\n",
            "| 32|blue-collar|  single|  primary|     no|    611|    yes|  no|       3|   -1|       0| unknown|    yes|     1.0|(11,[1],[1.0])|         1.0|(2,[1],[1.0])|           2.0|(3,[2],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[32.0,611.0,-1.0,...|\n",
            "| 49|   services| married|secondary|     no|     -8|    yes|  no|       1|   -1|       0| unknown|    yes|     4.0|(11,[4],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[49.0,-8.0,-1.0,0.0]|\n",
            "| 41|     admin.| married|secondary|     no|     55|    yes|  no|       2|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         0.0|(2,[0],[1.0])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      0.0|(1,[0],[1.0])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[41.0,55.0,-1.0,0.0]|\n",
            "| 49|     admin.|divorced|secondary|     no|    168|    yes| yes|       1|   -1|       0| unknown|    yes|     3.0|(11,[3],[1.0])|         2.0|    (2,[],[])|           0.0|(3,[0],[1.0])|         0.0|(1,[0],[1.0])|         1.0|    (1,[],[])|      1.0|    (1,[],[])|          0.0|(3,[0],[1.0])|         1.0| (1,[],[])|[49.0,168.0,-1.0,...|\n",
            "+---+-----------+--------+---------+-------+-------+-------+----+--------+-----+--------+--------+-------+--------+--------------+------------+-------------+--------------+-------------+------------+-------------+------------+-------------+---------+-------------+-------------+-------------+------------+----------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cfL6_sca5VwI",
        "outputId": "22059631-e94b-4d06-951b-f3535cea4406",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 754
        }
      },
      "source": [
        "# then apply Min-Max normalisation on each attribute using MinMaxScaler  \n",
        "from pyspark.ml.feature import MinMaxScaler\n",
        "  \n",
        "scaler = MinMaxScaler(inputCol=\"nums\", outputCol=\"scaledNums\")\n",
        "\n",
        "# Compute summary statistics and generate MinMaxScalerModel\n",
        "scalerModel = scaler.fit(df2)\n",
        "\n",
        "# rescale each feature to range [min, max].\n",
        "df2 = scalerModel.transform(df2)\n",
        "\n",
        "df2.show(23)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IllegalArgumentException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1257.fit.\n: java.lang.IllegalArgumentException: requirement failed: Output column scaledNums already exists.\n\tat scala.Predef$.require(Predef.scala:224)\n\tat org.apache.spark.ml.feature.MinMaxScalerParams$class.validateAndTransformSchema(MinMaxScaler.scala:68)\n\tat org.apache.spark.ml.feature.MinMaxScaler.validateAndTransformSchema(MinMaxScaler.scala:93)\n\tat org.apache.spark.ml.feature.MinMaxScaler.transformSchema(MinMaxScaler.scala:129)\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\n\tat org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:119)\n\tat org.apache.spark.ml.feature.MinMaxScaler.fit(MinMaxScaler.scala:93)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-01813f2968fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Compute summary statistics and generate MinMaxScalerModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mscalerModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# rescale each feature to range [min, max].\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/spark-2.4.0-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIllegalArgumentException\u001b[0m: 'requirement failed: Output column scaledNums already exists.'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9trUY7ZgCzhW"
      },
      "source": [
        "## 4. Unsupervised learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KTQfUch2Cmmi"
      },
      "source": [
        "### 4.1 K-means"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGVqbIXnK1xz",
        "colab_type": "code",
        "outputId": "4c3db9cf-c4b3-4d69-c407-f081b6a33573",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "df2 = df.selectExpr('scaledNums as features')\n",
        "df2.show() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|            features|\n",
            "+--------------------+\n",
            "|[0.53246753246753...|\n",
            "|[0.49350649350649...|\n",
            "|[0.29870129870129...|\n",
            "|[0.48051948051948...|\n",
            "|[0.46753246753246...|\n",
            "|[0.31168831168831...|\n",
            "|[0.49350649350649...|\n",
            "|[0.54545454545454...|\n",
            "|[0.24675324675324...|\n",
            "|[0.12987012987012...|\n",
            "|[0.25974025974025...|\n",
            "|[0.15584415584415...|\n",
            "|[0.14285714285714...|\n",
            "|[0.36363636363636...|\n",
            "|[0.16883116883116...|\n",
            "|[0.22077922077922...|\n",
            "|[0.18181818181818...|\n",
            "|[0.40259740259740...|\n",
            "|[0.29870129870129...|\n",
            "|[0.40259740259740...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dGGZI70Ohqgg",
        "outputId": "49129dd4-8c6d-480a-aaf2-42b92dcaf3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# Perform unsupervised learning on df2 with k-means \n",
        "# you can use whole df2 as both training and testing data, \n",
        "# evaluate the clustering result using Accuracy.  \n",
        "\n",
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "\n",
        "# Trains a k-means model.\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(df2)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(df2)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "\n",
        "# Shows the result.\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.5585376851872408\n",
            "Cluster Centers: \n",
            "[0.47145453 0.09911939 0.05347191 0.01343832]\n",
            "[0.20494051 0.09284213 0.06561405 0.01487672]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FHom8o2KCt36"
      },
      "source": [
        "### 4.2 PCA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wT4cx5uGTjmj",
        "outputId": "dd2e72ad-0af0-4d2c-b922-8de52f2e8587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "# generate a scatter plot using the first two PCA components to investigate the data distribution.\n",
        " \n",
        "from pyspark.ml.feature import PCA, StandardScaler\n",
        "from pyspark.ml.linalg import Vectors\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "standardizer = StandardScaler(withMean=True, withStd=True,\n",
        "                              inputCol='features',\n",
        "                              outputCol='std_features')\n",
        "model = standardizer.fit(df2)\n",
        "df2 = model.transform(df2)\n",
        "\n",
        "pca = PCA(k=2, inputCol=\"std_features\", outputCol=\"pcaFeatures\")\n",
        "model = pca.fit(df2)\n",
        "\n",
        "result = model.transform(df2).select(\"pcaFeatures\")\n",
        "result.show(truncate=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------------------------------------------+\n",
            "|pcaFeatures                               |\n",
            "+------------------------------------------+\n",
            "|[0.5024480111418868,1.2876640294358876]   |\n",
            "|[0.5696986581689144,0.6104554195913101]   |\n",
            "|[0.6016201800130624,-0.014764689663041394]|\n",
            "|[0.5155958010388783,1.0788875953172095]   |\n",
            "|[0.5745372759476424,0.521809213003675]    |\n",
            "|[0.6279346675693007,-0.23110756666275342] |\n",
            "|[0.5509097950203947,0.7809033950435555]   |\n",
            "|[0.541400103465537,0.956675889966385]     |\n",
            "|[0.6483246401447347,-0.5279590366276407]  |\n",
            "|[0.5632651836351033,0.04229805083768032]  |\n",
            "|[0.6418723077642554,-0.4470493296857937]  |\n",
            "|[0.6695321744365385,-0.8769786751215933]  |\n",
            "|[0.6762477902496722,-0.9602768250315613]  |\n",
            "|[0.6005935071664849,0.10642765655551936]  |\n",
            "|[0.6560190590894179,-0.7320152703983291]  |\n",
            "|[0.5646760896940652,0.18612872525217847]  |\n",
            "|[0.6541382844941198,-0.6925776186302129]  |\n",
            "|[0.5995466760232857,0.1830514255072168]   |\n",
            "|[0.6307010318926183,-0.2785790720509119]  |\n",
            "|[0.5953341411008151,0.22126651299714692]  |\n",
            "+------------------------------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ulp_uILXCv4Z"
      },
      "source": [
        "## 5. Supervised learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7-RorKELakx",
        "colab_type": "code",
        "outputId": "6029443b-63e2-4d62-d575-45e5511d5489",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "df2 = df.selectExpr('scaledNums as features', 'poutcomeIndex as label')\n",
        "df2.show() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+\n",
            "|            features|label|\n",
            "+--------------------+-----+\n",
            "|[0.53246753246753...|  0.0|\n",
            "|[0.49350649350649...|  0.0|\n",
            "|[0.29870129870129...|  0.0|\n",
            "|[0.48051948051948...|  0.0|\n",
            "|[0.46753246753246...|  0.0|\n",
            "|[0.31168831168831...|  0.0|\n",
            "|[0.49350649350649...|  0.0|\n",
            "|[0.54545454545454...|  0.0|\n",
            "|[0.24675324675324...|  0.0|\n",
            "|[0.12987012987012...|  0.0|\n",
            "|[0.25974025974025...|  0.0|\n",
            "|[0.15584415584415...|  0.0|\n",
            "|[0.14285714285714...|  0.0|\n",
            "|[0.36363636363636...|  0.0|\n",
            "|[0.16883116883116...|  0.0|\n",
            "|[0.22077922077922...|  0.0|\n",
            "|[0.18181818181818...|  0.0|\n",
            "|[0.40259740259740...|  0.0|\n",
            "|[0.29870129870129...|  0.0|\n",
            "|[0.40259740259740...|  0.0|\n",
            "+--------------------+-----+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0O7tszcPfnHN",
        "outputId": "a8185d49-f668-4725-a24d-257309fd71b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train, test = df2.randomSplit([0.7, 0.3], seed = 742)\n",
        "print(\"Training Dataset Count: \" + str(train.count()))\n",
        "print(\"Test Dataset Count: \" + str(test.count()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Count: 7822\n",
            "Test Dataset Count: 3340\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2SsHdh7YC-eN"
      },
      "source": [
        "### 5.1 LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vqo_ywFQYxSj",
        "colab": {}
      },
      "source": [
        "# Logistic Regression\n",
        "\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator \n",
        "\n",
        "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(train)\n",
        "\n",
        "\n",
        "# We can also use the multinomial family for binary classification\n",
        "mlr = LogisticRegression(maxIter=10,\n",
        "                         regParam=0.3,\n",
        "                         elasticNetParam=0.8, \n",
        "                         family=\"multinomial\")\n",
        "\n",
        "# Fit the model\n",
        "mlrModel = mlr.fit(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hANwFUzhgG83",
        "outputId": "4134ce7d-4bd8-48f9-f241-695e6e5b168a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# exam the coefficients\n",
        "# Print the coefficients and intercepts for logistic regression with multinomial family\n",
        "print(\"Multinomial coefficients: \" + str(mlrModel.coefficientMatrix))\n",
        "print(\"Multinomial intercepts: \" + str(mlrModel.interceptVector))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Multinomial coefficients: 4 X 4 CSCMatrix\n",
            "(0,1) -3.3467\n",
            "Multinomial intercepts: [1.8584058718122476,-0.29277621249950403,-0.4539798385650064,-1.111649820747737]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "evM5eiJoDHw2"
      },
      "source": [
        "### 5.2 Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "He4mlHb7hBoY",
        "outputId": "4f8bb56b-8a50-43ad-a5ea-114f5649f90a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Decision tree\n",
        "\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "dt = DecisionTreeClassifier(labelCol=\"label\", \n",
        "                            featuresCol=\"features\")\n",
        "\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = dt.fit(train)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(test)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       0.0|  0.0|[0.0,0.0777958228...|\n",
            "|       0.0|  0.0|[0.0,0.0789883135...|\n",
            "|       2.0|  2.0|[0.0,0.0846668408...|\n",
            "|       0.0|  0.0|[0.01298701298701...|\n",
            "|       0.0|  0.0|[0.01298701298701...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Error = 0.115868 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CaE-Z_IlDKXF"
      },
      "source": [
        "### 5.3 NaiveBayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSKBCIAQMuc6",
        "colab_type": "code",
        "outputId": "be465d2e-74c5-4dea-bb68-5530575ee438",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "# create the trainer and set its parameters\n",
        "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
        "\n",
        "# train the model\n",
        "model = nb.fit(train)\n",
        "\n",
        "# select example rows to display.\n",
        "predictions = model.transform(test)\n",
        "predictions.show()\n",
        "\n",
        "# compute accuracy on the test set\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
        "                                              metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test set accuracy = \" + str(accuracy))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "|            features|label|       rawPrediction|         probability|prediction|\n",
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "|[0.0,0.0777958228...|  0.0|[-0.4026581009425...|[0.75514649558636...|       0.0|\n",
            "|[0.0,0.0789883135...|  0.0|[-0.4043642558999...|[0.75526616923914...|       0.0|\n",
            "|[0.0,0.0846668408...|  2.0|[-1.3648044693040...|[0.58428306177620...|       0.0|\n",
            "|[0.01298701298701...|  0.0|[-0.4115222093901...|[0.75684612238886...|       0.0|\n",
            "|[0.01298701298701...|  0.0|[-0.4122209204679...|[0.75689490078114...|       0.0|\n",
            "|[0.01298701298701...|  0.0|[-0.4125784005542...|[0.75691985456091...|       0.0|\n",
            "|[0.02597402597402...|  0.0|[-0.4109780919298...|[0.75788310163379...|       0.0|\n",
            "|[0.02597402597402...|  0.0|[-0.4118555430507...|[0.75794417488597...|       0.0|\n",
            "|[0.02597402597402...|  0.0|[-0.4122292722319...|[0.75797018433582...|       0.0|\n",
            "|[0.02597402597402...|  0.0|[-0.4166165278366...|[0.75827536828657...|       0.0|\n",
            "|[0.02597402597402...|  0.0|[-0.4546556588392...|[0.76091026137691...|       0.0|\n",
            "|[0.03896103896103...|  0.0|[-0.4153087029191...|[0.75925499462749...|       0.0|\n",
            "|[0.03896103896103...|  0.0|[-0.4191597383944...|[0.75952189889337...|       0.0|\n",
            "|[0.03896103896103...|  0.0|[-0.4337514255539...|[0.76053134424912...|       0.0|\n",
            "|[0.03896103896103...|  0.0|[-0.4537378121977...|[0.76190920478302...|       0.0|\n",
            "|[0.03896103896103...|  2.0|[-2.3340911713411...|[0.40821075195877...|       0.0|\n",
            "|[0.05194805194805...|  0.0|[-0.4168932168818...|[0.76043160207860...|       0.0|\n",
            "|[0.05194805194805...|  0.0|[-0.4179169098563...|[0.76050232816888...|       0.0|\n",
            "|[0.05194805194805...|  0.0|[-0.4187293645979...|[0.76055844965455...|       0.0|\n",
            "|[0.05194805194805...|  0.0|[-0.4257977208500...|[0.76104632070959...|       0.0|\n",
            "+--------------------+-----+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Test set accuracy = 0.7889221556886228\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}